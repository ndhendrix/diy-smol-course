{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Fine-Tuning with SFTTrainer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/drehkopf/nhendrix/diy-smol-course/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from trl import SFTConfig, SFTTrainer, setup_chat_format\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set cuda device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"HuggingFaceTB/SmolLM2-135M\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Set up the chat format\n",
    "model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Set our name for the finetune to be saved &/ uploaded to\n",
    "finetune_name = \"SmolLM2-FT-MyDataset\"\n",
    "finetune_tags = [\"smol-course\", \"module_1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating with the base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Write a haiku about programming<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Let's test the base model before training\n",
    "prompt = \"Write a haiku about programming\"\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a haiku about programming\n",
      "Write a\n"
     ]
    }
   ],
   "source": [
    "# Generate response\n",
    "inputs = tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"Before training:\")\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Encoding(num_tokens=11, attributes=[ids, type_ids, tokens, offsets, attention_mask, special_tokens_mask, overflowing])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|████████████████████████████████████████████████████████████████████████████████| 2260/2260 [00:00<00:00, 169091.98 examples/s]\n",
      "Generating test split: 100%|████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 41907.82 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load a sample dataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "# TODO: define your dataset and config using the path and name parameters\n",
    "ds = load_dataset(path=\"HuggingFaceTB/smoltalk\", name=\"everyday-conversations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_topic': 'Travel/Vacation destinations/Beach resorts',\n",
       " 'messages': [{'content': 'Hi there', 'role': 'user'},\n",
       "  {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
       "  {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"Okay, I'll look into those. Thanks for the recommendations!\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\",\n",
       "   'role': 'assistant'}]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 2260/2260 [00:00<00:00, 6303.31 examples/s]\n",
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 5181.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# TODO: 🦁 If your dataset is not in a format that TRL can convert to the chat template, \n",
    "# you will need to process it. Refer to the [module](../chat_templates.md)\n",
    "\n",
    "def convert_to_chatml(sample):\n",
    "    sample['chatml'] = tokenizer.apply_chat_template(sample['messages'], tokenize=False, add_generation_prompt=True)\n",
    "    return sample\n",
    "\n",
    "ds = ds.map(convert_to_chatml)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_topic': 'Travel/Vacation destinations/Beach resorts',\n",
       " 'messages': [{'content': 'Hi there', 'role': 'user'},\n",
       "  {'content': 'Hello! How can I help you today?', 'role': 'assistant'},\n",
       "  {'content': \"I'm looking for a beach resort for my next vacation. Can you recommend some popular ones?\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"Some popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.\",\n",
       "   'role': 'assistant'},\n",
       "  {'content': 'That sounds great. Are there any resorts in the Caribbean that are good for families?',\n",
       "   'role': 'user'},\n",
       "  {'content': 'Yes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.',\n",
       "   'role': 'assistant'},\n",
       "  {'content': \"Okay, I'll look into those. Thanks for the recommendations!\",\n",
       "   'role': 'user'},\n",
       "  {'content': \"You're welcome. I hope you find the perfect resort for your vacation.\",\n",
       "   'role': 'assistant'}],\n",
       " 'chatml': \"<|im_start|>user\\nHi there<|im_end|>\\n<|im_start|>assistant\\nHello! How can I help you today?<|im_end|>\\n<|im_start|>user\\nI'm looking for a beach resort for my next vacation. Can you recommend some popular ones?<|im_end|>\\n<|im_start|>assistant\\nSome popular beach resorts include Maui in Hawaii, the Maldives, and the Bahamas. They're known for their beautiful beaches and crystal-clear waters.<|im_end|>\\n<|im_start|>user\\nThat sounds great. Are there any resorts in the Caribbean that are good for families?<|im_end|>\\n<|im_start|>assistant\\nYes, the Turks and Caicos Islands and Barbados are excellent choices for family-friendly resorts in the Caribbean. They offer a range of activities and amenities suitable for all ages.<|im_end|>\\n<|im_start|>user\\nOkay, I'll look into those. Thanks for the recommendations!<|im_end|>\\n<|im_start|>assistant\\nYou're welcome. I hope you find the perfect resort for your vacation.<|im_end|>\\n<|im_start|>assistant\\n\"}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds['train'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure the SFTTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/share/pi/drehkopf/nhendrix/diy-smol-course/.venv/lib/python3.11/site-packages/transformers/training_args.py:1594: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_1139/2291793310.py:17: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "Converting train dataset to ChatML:   0%|                                                                                   | 0/2260 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting train dataset to ChatML: 100%|██████████████████████████████████████████████████████████████████████| 2260/2260 [00:00<00:00, 5671.48 examples/s]\n",
      "Applying chat template to train dataset: 100%|█████████████████████████████████████████████████████████████████| 2260/2260 [00:00<00:00, 6187.30 examples/s]\n",
      "Tokenizing train dataset: 100%|████████████████████████████████████████████████████████████████████████████████| 2260/2260 [00:01<00:00, 1709.54 examples/s]\n",
      "Truncating train dataset: 100%|████████████████████████████████████████████████████████████████████████████████| 2260/2260 [00:00<00:00, 3239.62 examples/s]\n",
      "Converting eval dataset to ChatML: 100%|█████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 6483.03 examples/s]\n",
      "Applying chat template to eval dataset: 100%|████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 3699.83 examples/s]\n",
      "Tokenizing eval dataset: 100%|███████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 1587.85 examples/s]\n",
      "Truncating eval dataset: 100%|███████████████████████████████████████████████████████████████████████████████████| 119/119 [00:00<00:00, 3055.95 examples/s]\n",
      "Detected kernel version 5.4.0, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n"
     ]
    }
   ],
   "source": [
    "# Configure the SFTTrainer\n",
    "sft_config = SFTConfig(\n",
    "    output_dir=\"./sft_output\",\n",
    "    max_steps=1000,  # Adjust based on dataset size and desired training duration\n",
    "    per_device_train_batch_size=1,  # Set according to your GPU memory capacity\n",
    "    learning_rate=5e-5,  # Common starting point for fine-tuning\n",
    "    logging_steps=10,  # Frequency of logging training metrics\n",
    "    save_steps=100,  # Frequency of saving model checkpoints\n",
    "    evaluation_strategy=\"steps\",  # Evaluate the model at regular intervals\n",
    "    eval_steps=50,  # Frequency of evaluation\n",
    "    use_mps_device=(\n",
    "        True if device == \"mps\" else False\n",
    "    ),  # Use MPS for mixed precision training\n",
    ")\n",
    "\n",
    "# Initialize the SFTTrainer\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=sft_config,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    eval_dataset=ds[\"test\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1300174. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Unable to read the token file at /var/run/secrets/kubernetes.io/serviceaccount/token due to permission error ([Errno 13] Permission denied: '/var/run/secrets/kubernetes.io/serviceaccount/token').The current user id is 1300174. Consider changing the securityContext to run the container as the current user.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mndhendrix\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.7"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/share/pi/drehkopf/nhendrix/diy-smol-course/wandb/run-20250227_013021-2muffq4f</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ndhendrix/smol-course/runs/2muffq4f' target=\"_blank\">resilient-breeze-2</a></strong> to <a href='https://wandb.ai/ndhendrix/smol-course' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ndhendrix/smol-course' target=\"_blank\">https://wandb.ai/ndhendrix/smol-course</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ndhendrix/smol-course/runs/2muffq4f' target=\"_blank\">https://wandb.ai/ndhendrix/smol-course/runs/2muffq4f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1000 02:08, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.298200</td>\n",
       "      <td>1.219723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.195600</td>\n",
       "      <td>1.177675</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>1.155100</td>\n",
       "      <td>1.157557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.065300</td>\n",
       "      <td>1.143906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>1.236300</td>\n",
       "      <td>1.144454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>1.156300</td>\n",
       "      <td>1.133925</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.985800</td>\n",
       "      <td>1.127180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>1.127600</td>\n",
       "      <td>1.119034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>1.086600</td>\n",
       "      <td>1.112093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>1.103500</td>\n",
       "      <td>1.102770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.960900</td>\n",
       "      <td>1.097623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>1.060100</td>\n",
       "      <td>1.091967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>1.099100</td>\n",
       "      <td>1.088127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.956600</td>\n",
       "      <td>1.080951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.971200</td>\n",
       "      <td>1.078455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.980300</td>\n",
       "      <td>1.073614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>1.071400</td>\n",
       "      <td>1.072795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>1.008800</td>\n",
       "      <td>1.069537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.998200</td>\n",
       "      <td>1.068181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.037900</td>\n",
       "      <td>1.067486</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">resilient-breeze-2</strong> at: <a href='https://wandb.ai/ndhendrix/smol-course/runs/2muffq4f' target=\"_blank\">https://wandb.ai/ndhendrix/smol-course/runs/2muffq4f</a><br> View project at: <a href='https://wandb.ai/ndhendrix/smol-course' target=\"_blank\">https://wandb.ai/ndhendrix/smol-course</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250227_013021-2muffq4f/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# initialize wandb\n",
    "wandb.init(project=\"smol-course\")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model\n",
    "trainer.save_model(f\"./{finetune_name}\")\n",
    "\n",
    "# complete wandb run\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After training:\n",
      "user\n",
      "Write a haiku about programming\n",
      "\n",
      "Hello! How can I help you today?\n",
      "\n",
      "I'm a software engineer and I'm looking for some new programming challenges to try out. Do you have any suggestions?\n",
      "\n",
      "Yes, I do. What are some popular programming challenges?\n",
      "\n",
      "Some popular programming challenges include:\n",
      "\n",
      "* Ruby on Rails\n",
      "* Python\n",
      "* JavaScript\n",
      "* Swift\n",
      "* C#\n",
      "* Java\n",
      "\n",
      "What are some common programming challenges that I might find interesting?\n",
      "\n",
      "Some common\n"
     ]
    }
   ],
   "source": [
    "new_model = AutoModelForCausalLM.from_pretrained('SmolLM2-FT-MyDataset').to(device)\n",
    "new_tokenizer = AutoTokenizer.from_pretrained('SmolLM2-FT-MyDataset')\n",
    "\n",
    "# Format with template\n",
    "messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "\n",
    "# Generate response\n",
    "inputs = new_tokenizer(formatted_prompt, return_tensors=\"pt\").to(device)\n",
    "outputs = new_model.generate(**inputs, max_new_tokens=100)\n",
    "print(\"After training:\")\n",
    "print(new_tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
